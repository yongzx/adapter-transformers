

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction to Adapters &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Adapter Activation and Composition" href="adapter_composition.html" />
    <link rel="prev" title="Quickstart" href="quickstart.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">adapter-transformers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction to Adapters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adapter-types">Adapter types</a></li>
<li class="toctree-l2"><a class="reference internal" href="#adapter-architectures">Adapter architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="v2_transition.html">Transitioning from v1 to v2</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Hub</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="huggingface_hub.html">Integration with HuggingFace’s Model Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_layer.html">AdapterLayerBaseMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_utils.html">Adapter Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/weights_loaders.html">Weights Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xlmroberta.html">XLM-RoBERTa</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Introduction to Adapters</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/adapters.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="introduction-to-adapters">
<h1>Introduction to Adapters<a class="headerlink" href="#introduction-to-adapters" title="Permalink to this headline">¶</a></h1>
<p>Adapters have been introduced as a new alternative of fine-tuning language models on a downstream task.
Instead of fine-tuning the full model, a small set of newly introduced task-specific parameters is updated during fine-tuning.
The rest of the model is kept fix.
Adapters provide advantages in terms of size, modularity and composability while often achieving results on-par with full fine-tuning.
We will not go into detail about the theoretical background of adapters in the following but refer to some literature providing more explanations here:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Parameter-Efficient Transfer Learning for NLP</a> (Houlsby et al., 2019)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1909.08478.pdf">Simple, Scalable Adaptation for Neural Machine Translation</a> (Bapna and Firat, 2019)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</a> (Pfeiffer et al., 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2007.07779.pdf">AdapterHub: A Framework for Adapting Transformers</a> (Pfeiffer et al., 2020)</p></li>
</ul>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. note::
    *AdapterHub* aims to support a variety adapter setups. To understand our implementation of adapters, there&#39;s some important terminology to grasp at first: the difference between *adapter types* and *adapter architectures*. 
    
    The *adapter type* describes for which purpose an adapter is used whereas the *adapter architecture* (or *adapter configuration*) describes from which components the adapter modules in the language model are constructed.
</pre></div>
</div>
<div class="section" id="adapter-types">
<h2>Adapter types<a class="headerlink" href="#adapter-types" title="Permalink to this headline">¶</a></h2>
<p>The adapter type defines the purpose of an adapter. Currently, all adapters are categorized as one of the following types:</p>
<ul class="simple">
<li><p><strong>Task adapter</strong>: Task adapters are fine-tuned to learn representations for a specific downstream tasks such as sentiment analysis, question answering etc. Task adapters for NLP were first introduced by <a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Houlsby et al., 2019</a>.</p></li>
<li><p><strong>Language adapter</strong>: Language adapters are used to learn language-specific transformations. After being trained on a language modeling task, a language adapter can be stacked before a task adapter for training on a downstream task. To perform zero-shot cross-lingual transfer, one language adapter can simply be replaced by another. In terms of architecture, language adapters are largely similar to task adapters, except for an additional <em>invertible adapter</em> layer after the embedding layer. This setup was introduced and is further explained by <a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">Pfeiffer et al., 2020</a>.</p></li>
</ul>
<p>Beginning with version 2, both adapter types are treated identically within the library.
The additional invertible adapters are defined via the adapter configuration (see next section).
In v1.x, the distinction between task and language adapters was made with the help of the <code class="docutils literal notranslate"><span class="pre">AdapterType</span></code> enumeration.</p>
</div>
<div class="section" id="adapter-architectures">
<h2>Adapter architectures<a class="headerlink" href="#adapter-architectures" title="Permalink to this headline">¶</a></h2>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/architecture.png
    :width: 350
    :align: center
    :alt: Adapter architectures

    Visualization of possible adapter configurations with corresponding dictionary keys.
</pre></div>
</div>
<p>The concrete structure of adapter modules and their location in the layers of a Transformer model is specified by a configuration dictionary.
This is referred to as the adapter architecture.
The currently possible configuration options are visualized in the figure above.
When adding new adapters using the <code class="docutils literal notranslate"><span class="pre">add_adapter()</span></code> method, the configuration can be set providing the <code class="docutils literal notranslate"><span class="pre">config</span></code> argument.
The passed value can be either a plain Python dict containing all keys pictured or a subclass of <span class="xref myst"><code class="docutils literal notranslate"><span class="pre">AdapterConfig</span></code></span>.</p>
<p>For convenience, <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> has some common architectures built-in:</p>
<ul class="simple">
<li><p><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">HoulsbyConfig</span></code></span> as proposed by <a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Houlsby et al., 2019</a></p></li>
<li><p><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">PfeifferConfig</span></code></span> as proposed by <a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">Pfeiffer et al., 2020</a></p></li>
</ul>
<p>Both of these classes have counterparts with invertible adapters, typically used as language adapters:
<span class="xref myst"><code class="docutils literal notranslate"><span class="pre">HoulsbyInvConfig</span></code></span> and <span class="xref myst"><code class="docutils literal notranslate"><span class="pre">PfeifferInvConfig</span></code></span>.</p>
<p>Furthermore, pre-defined architectures can be loaded from the Hub:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load &quot;pfeiffer&quot; config from Hub, but replace the reduction factor</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AdapterConfig</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;pfeiffer&quot;</span><span class="p">,</span> <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># add a new adapter with the loaded config</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;dummy&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also <span class="xref myst">add your own architecture to the Hub</span>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="adapter_composition.html" class="btn btn-neutral float-right" title="Adapter Activation and Composition" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="quickstart.html" class="btn btn-neutral float-left" title="Quickstart" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020-2022, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: v2.3.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt><dd><a href="../v1.1.1/index.html">v1.1.1</a></dd><dd><a href="adapters.html">v2.3.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../index.html">master</a></dd>
    </dl>
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>